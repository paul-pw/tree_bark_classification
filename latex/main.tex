\documentclass{article}
\usepackage[final]{nips_2017}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[ngerman]{babel}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{pgfplots}
\pgfplotsset{width=0.8\linewidth}
\title{Machine Learning for Computer Vision Abschlussprojekt: Rindenklassifizierung}

\author{
  Paul Walker\\
  Department of Computer Science\\
  DHBW Stuttgart\\
  \texttt{inf20045@dhbw-stuttgart.de} \\
  \And
  Tom Hofer\\
  Department of Computer Science\\
  DHBW Stuttgart\\
  \texttt{inf20173@lehre-dhbw-stuttgart.de} \\
}

\begin{document}

\begin{center}
\includegraphics[height=0.7cm]{DHBW_logo}
\end{center}

\maketitle

\begin{abstract}
  % The abstract should consist of 1 paragraph describing the motivation for your paper and a high-level explanation of the methodology you used/results obtained.
  Diese Arbeit befasst sich mit der Entwicklung eines Machine Learning Modells zur Klassifikation von Rinde zu bestimmten Baumarten. Das Modell könnte in der Praxis beispielsweise in einer App zur Bestimmung der Baumart anhand eines Fotos der Baumrinde eingesetzt werden. Im Rahmen der Arbeit werden vier Modelle basierend auf den vortrainierten Modellen InceptionV3, MobileNetV2, VGG16 und VGG19 erstellt, getestet und gegenübergestellt. Für das Training der Modelle kommen ein Trainingsset mit Daten aus dem Internet und ein Validationset mit selbst erhobenen Daten zum Einsatz.
\end{abstract}

\section{Einleitung}
%Explain the problem and why it is important. Discuss your motivation for pursuing this
%problem. Give some background if necessary. Clearly state what the input and output
%is. Be very explicit: “The input to our algorithm is an {image, amplitude, patient age,
%rainfall measurements, grayscale video, etc.}. We then use a {SVM, neural network, linear
%regression, etc.} to output a predicted {age, stock price, cancer type, music genre, etc.}.”
%This is very important since different teams have different inputs/outputs spanning different
%application domains. Being explicit about this makes it easier for readers. If you are using
%your project for multiple classes, add a paragraph explaining which components of the
%project were used for each class.
Unser Abschlussprojekt befasst sich mit der Entwicklung eines Machine Learning Modells zur Bestimmung der Baumart anhand eines Fotos der Baumrinde. Die Rinde eignet sich sehr gut für die Bestimmung der Baumart, da diese, anders als zum Beispiel Blätter oder Früchte, unabhängig von der Jahreszeit ist. Die Eingabe für unseren Algorithmus ist also ein Bild. Zur Klassifizierung der Baumart verwenden wir ein vortrainiertes Convolutional Neural Network, welches wir für unseren Anwendungsfall anpassen. Die Ausgabe, beziehungsweise Vorhersage unseres Algorithmus ist die Art des Baumes, dessen Rinde auf dem Foto abgebildet ist.

In der Praxis könnte ein solches Modell zum Beispiel in einer App eingesetzt werden, die Baumarten bestimmen soll. Das Erkennen der Baumart anhand der Rinde ist ein schwieriges Problem, da es teilweise gro\ss e Varianzen innerhalb einer Klasse und kleine klassenübergreifende Varianzen gibt. Abbildung~\ref{problem} zeigt eine Kiefer die sehr ähnlich zu einer Lärche aussieht und eine weitere Lärche die sich von den anderen beiden Bäumen relativ stark unterscheidet.

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=0.32\linewidth]{examples/Kiefer}
  \includegraphics[width=0.32\linewidth]{examples/Lärche1}
  \includegraphics[width=0.32\linewidth]{examples/Lärche2}\\
  Kiefer \hspace{100px} Lärche \hspace{100px} Lärche
  \caption{Schwierigkeit des Problems}
  \label{problem}
\end{figure}

\section{Stand der Technik}
%[Optional and seen as extension] You can find existing papers, and see how they are similar to and differ
%from your work. In your opinion, which approaches were clever/good? What is the stateof-the-art?
%Do most people perform the task by hand? This includes previous attempts by others at your problem,
%previous technical methods, or previous learning algorithms. Google Scholar is very useful
%for this: https://scholar.google.com/ (you can click “cite” and it generates MLA, APA,
%BibTeX, etc.)
Die Rindenklassifikation ist kein neues Problem und es wurden bereits Ansätze sowohl mit klassischer Computer Vision, als auch mit Hilfe von Deep Learning entwickelt. [1]

\section{Daten und Features}
%Describe your dataset: how many training/validation/test examples do you have? Is there
%any preprocessing you did? What about normalization or data augmentation? What is the
%resolution of your images? How is your time-series data discretized? Include a citation on
%where you obtained your dataset from. Depending on available space, show some examples
%from your dataset. You should also talk about the features you used. If you extracted
%features using Fourier transforms, word2vec, PCA,
%ICA, etc. make sure to talk about it. Try to include examples of your data in the report
%(e.g. include an image, show a waveform, etc.).

Unser Trainingsdatenset besteht aus 21491 Bildern von Rinde aus dem Internet. Die Daten stammen aus dem BARK-KR~[2] Datenset, dem Tree Species Dataset~[3] und dem BarkNet~1.0~[4] Datenset. Einige Beispile aus den Trainingsdaten sind in Abbildung~\ref{train} abgebildet. Für das Validationset haben wir 333 eigene Fotos aufgenommen. Einige Beispiele sind in Abbildung~\ref{validation} abgebildet. Ein Testset haben wir nicht, wir können also keine genaue Aussage über die tatsächliche Feldperformance unseres Modells treffen. Alle Bilder werden auf eine Grö\ss e von 512*512 Pixeln gebracht. Um Verzerrungen zu vermeiden werden die Grafiken zunächst durch Zuschneiden auf das richtige Seitenverhältnis gebracht. In unserem Validationset haben wir acht Klassen, beziehungsweise acht Baumarten. Wir nutzen von den Trainingsdaten nur die Arten, die auch im Validationset vorkommen. Zusätzlich wurden die Label der Trainingsdaten angepasst, da diese die Bäume noch in genaue unterarten einteilen, wir interessieren uns jedoch nur für die Überart. So wird beispielsweise die Orientalische Wei\ss -Eiche Quercus aliena in unserem Datensatz einfach als Oak, also Eiche, klassifiziert. Die vertretenen Baumarten sind Esche, Buche, Birke, Tanne, Lärche, Eiche, Kiefer und Fichte.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.24\linewidth]{examples/train/Ash}
  \includegraphics[width=0.24\linewidth]{examples/train/Beech}
  \includegraphics[width=0.24\linewidth]{examples/train/Birch}
  \includegraphics[width=0.24\linewidth]{examples/train/Fir}
  Esche \hspace{70pt} Buche \hspace{70pt} Birke \hspace{70pt} Tanne\\
  \includegraphics[width=0.24\linewidth]{examples/train/Larch}
  \includegraphics[width=0.24\linewidth]{examples/train/Oak}
  \includegraphics[width=0.24\linewidth]{examples/train/Pine}
  \includegraphics[width=0.24\linewidth]{examples/train/Spruce}
  Lärche \hspace{70pt} Eiche \hspace{70pt} Kiefer \hspace{70pt} Fichte\\
  \caption{Beispiele aus den Trainingsdaten}
  \label{train}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.24\linewidth]{examples/validation/Ash}
  \includegraphics[width=0.24\linewidth]{examples/validation/Beech}
  \includegraphics[width=0.24\linewidth]{examples/validation/Birch}
  \includegraphics[width=0.24\linewidth]{examples/validation/Fir}
  Esche \hspace{70pt} Buche \hspace{70pt} Birke \hspace{70pt} Tanne\\
  \includegraphics[width=0.24\linewidth]{examples/validation/Larch}
  \includegraphics[width=0.24\linewidth]{examples/validation/Oak}
  \includegraphics[width=0.24\linewidth]{examples/validation/Pine}
  \includegraphics[width=0.24\linewidth]{examples/validation/Spruce}
  Lärche \hspace{70pt} Eiche \hspace{70pt} Kiefer \hspace{70pt} Fichte\\
  \caption{Baspiele aus dem Validationset}
  \label{validation}
\end{figure}

\section{Methoden}
%Describe your learning algorithms, proposed algorithm(s), or theoretical proof(s). Make
%sure to include relevant mathematical notation. For example, you can include the loss function you are using. It is okay to use formulas from the lectures (online or in-class). For each algorithm, give a short description 
%of how it works. Again, we are looking for your understanding of how these deep
%learning algorithms work. Although the teaching staff probably know the algorithms, future
%readers may not. Additionally, if you are
%using a niche or cutting-edge algorithm (anything else not covered in the class), you may want to explain your algorithm using 1/2
%paragraphs. Note: Theory/algorithms projects may have an appendix showing extended
%proofs (see Appendix section below).

Zur Erstellung unseres Modells verwenden wir Transfer learning. Das bedeutet wir verwenden ein bereits für Bilddaten Trainiertes Modell aus dem Internet und passen dieses auf unseren Anwendungsfall an. Die Anpassung erfolgt in den Ersten und letzten Schichten des Modells. Die erste Schicht wird auf unsere Eingabe, ein Bild mit 512*512 Pixeln und drei Kanälen, angepasst. Diese hinzugefügten Schichten sind beispielhaft in dem in Abbildung~\ref{VGG16} dargestelleten, auf VGG16 basierenden Modell, grün markiert. Au\ss erdem wird eine zweite Schicht eingefügt die ein Rescaling der Wertebereiche von 0 bis 255 auf einen Wertebereich von 0 bis 1 transformiert. Für die Ausgabe des Modells werden die letzten vier Shichten angepasst. Zunächst erfogt ein zeidimensionales global average pooling gefolgt von einer flatten und dense Schicht. Die dense Schicht verwendet eine Relu als Aktivierungsfunktion. In der letzten Schicht wird die Ausgabe des Modells nochmal auf die Anzahl an Klassen, also in unserem Fall acht, heruntergebrochen. Die Letzte Schicht ist ein Softmax-Layer. Diese Schichten sind in der Abbildung~\ref{VGG16} rot markiert.

Wir verweden für unser Modell einen sparse categorial crossentropy loss.

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=0.4\linewidth]{VGG16_top}
  \includegraphics[width=0.4\linewidth]{VGG16_bottom}
  \caption{Darstellung des auf VGG16 basierenden Modells}
  \label{VGG16}
\end{figure}

\FloatBarrier

\section{Experimente/Ergebnisse/Diskussion}
%You should also give details about what (hyper)parameters you chose (e.g. why did you
%use X learning rate for gradient descent, what was your mini-batch size and why) and how
%you chose them. What your primary metrics are: accuracy, precision,
%AUC, etc. Provide equations for the metrics if necessary. For results, you want to have a
%mixture of tables and plots. If you are solving a classification problem, can include a
%confusion matrix. Include performance metrics such as precision,
%recall, and accuracy. For regression problems, state the average error. You should have
%both quantitative and qualitative results. To reiterate, you must have both quantitative
%and qualitative results! If it applies: include visualizations of results, heatmaps,
%examples of where your algorithm failed and a discussion of why certain algorithms failed
%or succeeded. In addition, explain whether you think you have overfit to your training set
%and what, if anything, you did to mitigate that. Make sure to discuss the figures/tables in
%your main text throughout this section. Your plots should include legends, axis labels, and
%have font sizes that are legible when printed.

Wir haben verschiedene vortrainierte Basismodelle ausprobiert und deren Performance gegenübergestellt. Die betrachteten vortrainierten Modelle sind InceptionV3, MobileNetV2, VGG16 und VGG19. Die Ergebnisse der Gegenüberstellung sind in Abbildung~\ref{vergleich} dargestellt. Die höchste Performance erzielt MobileNetV2 mit einer Trefferate von 52,85\% auf dem Devset mit unseren selbst gelabelten Daten. Bei acht Klassen würde man mit Raten auf eine Trefferate von 12,5\% kommen, das Modell ist also deutlich besser als Raten. Da alle Modelle jedoch auf unseren Daten eine sehr viel geringere Trefferchance haben als beim Taining, haben wir ein weiteres Devset mit Daten aus dem Internet erstellt. Auf diesen Daten weisen die Modelle eine ähnliche Trefferrate wie beim Training auf. Unsere Modelle können also nicht gut von den Trainingsdaten auf unsere eigenen Daten generalisieren. Wir vermuten die Ursache in den regional verschiedenen Unterarten. Um ein besseres Ergebnis zu erzielen müssten mehr regionale Daten erhoben werden, die dem Trainingsset hinzugefügt werden.

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      enlargelimits=0.25,
      legend style={at={(0.5,-0.13)},
        anchor=north,legend columns=-1},
      ylabel={accuracy in \%},
      symbolic x coords={InceptionV3,MobileNetV2,VGG16,VGG19},
      xtick=data,
      nodes near coords,
      nodes near coords style={font=\small, rotate=90, anchor=west}
      %nodes near coords align={vertical}
      ]
      \addplot % training
      coordinates {(InceptionV3,85.31) (MobileNetV2,90.55) (VGG16,76.02) (VGG19,69.61)};
      \addplot % validation internet
      coordinates {(InceptionV3,86.34) (MobileNetV2,88.82) (VGG16,75.16) (VGG19,70.50)};
      \addplot % validation self labeled
      coordinates {(InceptionV3,45.65) (MobileNetV2,52.85) (VGG16,28.83) (VGG19,24.62)};
      \legend{training,validation (internet), validation (self labeled)}
    \end{axis}
  \end{tikzpicture}
  \caption{Vergleich der verschiedenen Modelle}
  \label{vergleich}
\end{figure}

\FloatBarrier

\section{Fazit und Ausblick}
%Summarize your report and reiterate key points. Which algorithms were the highestperforming?
%Why do you think that some algorithms worked better than others? For
%future work, if you had more time, more team members, or more computational resources,
%what would you explore?

In dieser Arbeit wurden vier Neuronale Netze mittels Transfer Learning aus vier vortrainierten Modellen erstellt. Diese wurden mit Bildern von Rinde aus dem Internet trainiert, mit dem Ziel der Klassifikation zu einer von acht Baumarten. Zusätzlich wurden händisch Daten erhoben und gelabelt, die das Validationset bildeten. Die vier entwickelten Modelle wurden nach ihrer Treffwahrscheinlichkeit gegenübergestellt. Außerdem wurde eine schlechte Generalisierungsfähigkeit der Modelle von den externen auf die selbst erstellten Daten festgestellt.

Da die Performance auf einem aus den externen Daten erstellten Validationset sehr nahe an der Performance auf den Trainingsdaten liegt, kann davon ausgegangen werden, dass bei einem Training des Modells mit regionalen Daten eine ähnlich hohe Performance erreicht werden kann.


\section*{Referenzen}
%This section should include citations for: (1) Any papers mentioned in the related work
%section. (2) Papers describing algorithms that you used which were not covered in class.
%(3) Code or libraries you downloaded and used. This includes libraries such as scikit-learn, Tensorflow, Pytorch, Keras etc. Acceptable formats include: MLA, APA, IEEE. If you
%do not use one of these formats, each reference entry must include the following (preferably
%in this order): author(s), title, conference/journal, publisher, year. If you are using TeX,
%you can use any bibliography format which includes the items mentioned above. We are excluding
%the references section from the page limit to encourage students to perform a thorough
%literature review/related work section without being space-penalized if they include more
%references. Any choice of citation style is acceptable
%as long as you are consistent.
\subsection*{Verwendete Softwarebibliotheken}
\begin{itemize}
\item Pillow-SIMD
\item google.colab (drive)
\item Tensorflow
\item os
\item Keras
\item matplotlib
\end{itemize}

\medskip
\small
[1] Misra, D., Crispim-Junior, C., Tougne, L. (2020). Patch-Based CNN Evaluation for Bark Classification. In: Bartoli, A., Fusiello, A. (eds) Computer Vision – ECCV 2020 Workshops. ECCV 2020. Lecture Notes in Computer Science(), vol 12540. Springer, Cham. \url{https://doi.org/10.1007/978-3-030-65414-6_15}

[2] Fiel, Stefan, \& Sablatnig, Robert. (2010). Tree Species Dataset consisting of Images of the Bark, Leaves or Needles [Data set]. Zenodo. \url{https://doi.org/10.5281/zenodo.4446955}

[3] Fiel, Stefan, \& Sablatnig, Robert. (2010). Tree Species Dataset consisting of Images of the Bark, Leaves or Needles [Data set]. Zenodo. \url{https://doi.org/10.5281/zenodo.4446955}

[4] Giguère, Philippe; Carpentier, Mathieu; Gaudreault, Jonathan (2019), “BarkNet 1.0 (Part 1 of  4)”, Mendeley Data, V1, \url{doi: 10.17632/zgr7r2r4nt.1}

%[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
%for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
%T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
%  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

%[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
%  Exploring Realistic Neural Models with the GEneral NEural SImulation
%  System.}  New York: TELOS/Springer--Verlag.

%[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
%learning and recall at excitatory recurrent synapses and cholinergic
%modulation in rat hippocampal region CA3. {\it Journal of
%  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}